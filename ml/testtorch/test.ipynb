{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "99 810.5762939453125\n",
      "199 544.6072387695312\n",
      "299 367.09014892578125\n",
      "399 248.54281616210938\n",
      "499 169.32968139648438\n",
      "599 116.36717987060547\n",
      "699 80.93305206298828\n",
      "799 57.210445404052734\n",
      "899 41.317291259765625\n",
      "999 30.661800384521484\n",
      "1099 23.512327194213867\n",
      "1199 18.7115478515625\n",
      "1299 15.485221862792969\n",
      "1399 13.315179824829102\n",
      "1499 11.854280471801758\n",
      "1599 10.869909286499023\n",
      "1699 10.205991744995117\n",
      "1799 9.757780075073242\n",
      "1899 9.454900741577148\n",
      "1999 9.250006675720215\n",
      "Result: y = 0.013798343949019909 + 0.8409788608551025 x + -0.0023804439697414637 x^2 + -0.09108839929103851 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "# Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  -0.2 y:  0.64\n",
      "x:  -0.36000000000000004 y:  0.40959999999999996\n",
      "x:  -0.488 y:  0.26214400000000004\n",
      "x:  -0.5904 y:  0.16777215999999995\n",
      "x:  -0.67232 y:  0.10737418239999996\n",
      "x:  -0.7378560000000001 y:  0.06871947673599998\n",
      "x:  -0.7902848 y:  0.043980465111040035\n",
      "x:  -0.83222784 y:  0.028147497671065613\n",
      "x:  -0.865782272 y:  0.018014398509481944\n",
      "x:  -0.8926258176 y:  0.011529215046068408\n",
      "x:  -0.9141006540800001 y:  0.0073786976294838436\n",
      "x:  -0.931280523264 y:  0.004722366482869611\n",
      "x:  -0.9450244186112 y:  0.0030223145490365644\n",
      "x:  -0.95601953488896 y:  0.0019342813113834012\n",
      "x:  -0.9648156279111679 y:  0.0012379400392853457\n",
      "x:  -0.9718525023289344 y:  0.0007922816251426656\n",
      "x:  -0.9774820018631475 y:  0.0005070602400912838\n",
      "x:  -0.981985601490518 y:  0.0003245185536584483\n",
      "x:  -0.9855884811924144 y:  0.00020769187434144243\n",
      "x:  -0.9884707849539315 y:  0.00013292279957843878\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "lr=0.1\n",
    "epochs=20\n",
    "y=lambda x:x**2+x*2+1\n",
    "for epoch in range(epochs):\n",
    "    dx=2*x+2#梯度\n",
    "    x=x-lr*dx\n",
    "    print('x: ',x,'y: ',y(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad None data tensor([0.])\n",
      "grad of epoch0: tensor([2.])\n",
      "grad of epoch1: tensor([1.6000])\n",
      "grad of epoch2: tensor([1.2800])\n",
      "grad of epoch3: tensor([1.0240])\n",
      "grad of epoch4: tensor([0.8192])\n",
      "grad of epoch5: tensor([0.6554])\n",
      "grad of epoch6: tensor([0.5243])\n",
      "grad of epoch7: tensor([0.4194])\n",
      "grad of epoch8: tensor([0.3355])\n",
      "grad of epoch9: tensor([0.2684])\n",
      "grad of epoch10: tensor([0.2147])\n",
      "grad of epoch11: tensor([0.1718])\n",
      "grad of epoch12: tensor([0.1374])\n",
      "grad of epoch13: tensor([0.1100])\n",
      "grad of epoch14: tensor([0.0880])\n",
      "grad of epoch15: tensor([0.0704])\n",
      "grad of epoch16: tensor([0.0563])\n",
      "grad of epoch17: tensor([0.0450])\n",
      "grad of epoch18: tensor([0.0360])\n",
      "grad of epoch19: tensor([0.0288])\n",
      "tensor([-0.9885])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# 定义一个pytorch类型 且可自动求导的的初始值\n",
    "x = torch.Tensor([0])# 定义一个tensor，相当于np.array\n",
    "x = Variable(x,requires_grad=True) # x转变为一个variable，建立计算图的起点;开启requires_grad表示自动计算梯度\n",
    "print('grad',x.grad,'data',x.data) # grad表示x的梯度属性，表明当前累计的梯度；data表示tensor值\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 设置计算图:建立一个函数y，以x为变量\n",
    "    y = x ** 2 + 2 * x + 1\n",
    "    # Variable 能自动求导==》requires_grad\n",
    "    y.backward()  # 对y做反向传导==》自动计算梯度，由于当前变量为1个，所以不需要指定\n",
    "    print('grad of epoch' + str(epoch) + ':', x.grad.data)\n",
    "\n",
    "    x.data -= lr * x.grad.data\n",
    "    # 在 pytorch 中梯度会累积，则每次需要清0\n",
    "    x.grad.data.zero_()  # xx_表示对变量做inplace操作；此处将当前梯度清0\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2492307286934012\n"
     ]
    }
   ],
   "source": [
    "#numpy实现线性回归\n",
    "import numpy as np\n",
    "x_data = np.array([1, 2, 3])\n",
    "y_data = np.array([2, 4, 6])\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "w = 0\n",
    "cost = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 计算梯度\n",
    "\tyhat = x_data * w#设计的模型，用x预测y\n",
    "\tloss = np.average((yhat - y_data)**2)#最小二乘法求loss\n",
    "\tcost.append(loss)\n",
    "\tdw = np.dot(-2*(y_data - yhat),x_data.T)/(x_data.shape[0])\n",
    "    #参数更新\n",
    "\tw = w - lr*dw\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "##PyTorch实现线性回归\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#设置初始变量\n",
    "x_data = Variable(torch.Tensor([[1], [2], [3]]))\n",
    "y_data = Variable(torch.Tensor([[2], [4], [6]]))\n",
    "\n",
    "epochs = 20\n",
    "lr = 0.1\n",
    "w = Variable(torch.FloatTensor([0]), requires_grad=True)  # requires_grad一定不要忘记设置\n",
    "cost = []\n",
    "for epoch in range(epochs):\n",
    "    yhat=x_data*w\n",
    "    loss=torch.mean((yhat-y_data)**2)\n",
    "    cost.append(loss.data.numpy())  # tensor转化为ndarray\n",
    "    loss.backward()  # 计算loss偏导（仍用loss做目标优化函数）\n",
    "    #update w\n",
    "    w.data-=lr*w.grad.data \n",
    "    w.grad.data.zero_()#梯度置零，进入下一次\n",
    "print(w.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(14.6303), tensor(12.0267), tensor(9.8865), tensor(8.1272), tensor(6.6809), tensor(5.4920), tensor(4.5147), tensor(3.7112), tensor(3.0508), tensor(2.5079), tensor(2.0616), tensor(1.6947), tensor(1.3931), tensor(1.1452), tensor(0.9414), tensor(0.7739), tensor(0.6362), tensor(0.5230), tensor(0.4299), tensor(0.3534)]\n",
      "[Parameter containing:\n",
      "tensor([[1.7505]], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "#class 创建一个类，通过class model写一个神经网络类\n",
    "        # super 用来返回Model的父类，在pytorch下定义的类都是继承一个大的父类torch.nn.Module的父类。\n",
    "        # torch.nn.Module中包含了各种工具，一般我们都是写的都是子类，通过父类我们可以很容易书写子类。\n",
    "        \n",
    "        \n",
    "        # 建立一个linear类，bias表示偏置项,建立一个AX+b\n",
    "\n",
    "        # forward 是torch.nn.Module定义好的模板，表示前向传播\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.linear=torch.nn.Linear(1,1,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y_pred=self.linear(x)\n",
    "        return y_pred\n",
    "model=Model()\n",
    "criterion=torch.nn.MSELoss(reduction='mean')\n",
    "#其中的SGD就是optim中的一个算法（优化器）：随机梯度下降算法\n",
    "#PyTorch 的优化器基本都继承于 \"class Optimizer\"，这是所有 optimizer 的 base class\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "from torch.autograd import Variable\n",
    "torch.manual_seed(2)\n",
    "#设置初始变量\n",
    "x_data = Variable(torch.Tensor([[1], [2], [3]]))\n",
    "y_data = Variable(torch.Tensor([[2], [4], [6]]))\n",
    "\n",
    "epochs = 20\n",
    "cost = []\n",
    "for epoch in range(epochs):\n",
    "    #建立计算图\n",
    "    yhat=model(x_data)\n",
    "    loss=criterion(yhat,y_data)\n",
    "    cost.append(loss.data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #参数更新\n",
    "    optimizer.step()\n",
    "print(cost)\n",
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "批量梯度下降法(Batch Gradient Descent, BGD)：是梯度下降法的最原始形式，每迭代一步或更新每一参数时，都要用到训练集中的所有样本数据，当样本数目很多时，训练过程会很慢。\n",
    "\n",
    "随机梯度下降法(Stochastic Gradient Descent, SGD)：由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。随机梯度下降法正是为了解决批量梯度下降法这一弊端而提出的。随机梯度下降是通过每个样本来迭代更新一次。SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着最优化方向进行。\n",
    "\n",
    "小批量梯度下降法(Mini-BatchGradient Descent, MBGD)：在每次更新参数时使用m’个样本, m’可能远小于m。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0aef3ef50817fd087bf5afbc58c91c96bb4be0ad8b75e116879b1dad9351bb93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
